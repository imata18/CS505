{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tweepy\n",
    "import pickle\n",
    "import time\n",
    "import os\n",
    "import math\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.lm.preprocessing import pad_both_ends, padded_everygram_pipeline\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "from nltk.lm import KneserNeyInterpolated, MLE\n",
    "from twython import Twython\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from nltk.util import everygrams\n",
    "from functools import partial\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [],
   "source": [
    "#client = tweepy.Client(BEARER_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for tweet in tweepy.Paginator(client.search_recent_tweets, \"nft\", max_results=100, tweet_fields=['lang']).flatten(limit=10000):\n",
    "#    if(tweet.lang=='en'):\n",
    "#        tweetsArray.append(tweet.text)\n",
    "        \n",
    "#a_file = open(\"nftData.pkl\", \"wb\")\n",
    "#with a_file as f:\n",
    "#    pickle.dump(tweetsArray, f)\n",
    "\n",
    "#for tweet in tweepy.Paginator(client.search_recent_tweets, \"football\", max_results=100, tweet_fields=['lang']).flatten(limit=1000):\n",
    "#    if(tweet.lang=='en'):\n",
    "#        tweetsArray.append(tweet.text)\n",
    "        \n",
    "#a_file = open(\"footballData.pkl\", \"wb\")\n",
    "#with a_file as f:\n",
    "#    pickle.dump(tweetsArray, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [],
   "source": [
    "tknzr = TweetTokenizer(reduce_len=True, strip_handles=True)\n",
    "sentence_tokenizer=nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "with open('nftData.pkl', 'rb') as f:\n",
    "    train = pickle.load(f)\n",
    "     \n",
    "test = train[8000:]\n",
    "train = train[:8000]\n",
    "\n",
    "allData = []\n",
    "allTrainRegular = []\n",
    "allTrainLower = []\n",
    "allTrainTokens = []\n",
    "for tweet in train:\n",
    "    tweetSentences = []\n",
    "    for sentence in sent_tokenize(tweet):\n",
    "        sentenceInfo = []\n",
    "        sentenceInfo.append(sentence)\n",
    "        allTrainRegular.append(sentence)\n",
    "        sentenceInfo.append(sentence.lower())\n",
    "        allTrainLower.append(sentence.lower())\n",
    "        sentenceInfo.append(word_tokenize(sentence.lower()))\n",
    "        allTrainTokens.append(list(pad_both_ends(word_tokenize(sentence.lower()), n=3)))\n",
    "        tweetSentences.append(sentenceInfo)\n",
    "    allData.append(tweetSentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [],
   "source": [
    "allTestData = []\n",
    "allTestRegular = []\n",
    "allTestLower = []\n",
    "allTestTokens = []\n",
    "for tweet in test:\n",
    "    tweetSentences = []\n",
    "    for sentence in sent_tokenize(tweet):\n",
    "        sentenceInfo = []\n",
    "        sentenceInfo.append(sentence)\n",
    "        allTestRegular.append(sentence)\n",
    "        sentenceInfo.append(sentence.lower())\n",
    "        allTestLower.append(sentence.lower())\n",
    "        sentenceInfo.append(word_tokenize(sentence.lower()))\n",
    "        allTestTokens.append(list(pad_both_ends(word_tokenize(sentence.lower()), n=3)))  \n",
    "        tweetSentences.append(sentenceInfo)\n",
    "    allTestData.append(tweetSentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19337.0\n"
     ]
    }
   ],
   "source": [
    "ngram = 1\n",
    "train_data, vocab = padded_everygram_pipeline(ngram, allTrainTokens)\n",
    "test_data, _ = padded_everygram_pipeline(ngram, allTestTokens)\n",
    "model = KneserNeyInterpolated(ngram)\n",
    "model.fit(train_data, vocab)\n",
    "\n",
    "totalPerpScore = 0\n",
    "totalSentences = 0\n",
    "\n",
    "for i in range(len(test)):\n",
    "    try:\n",
    "        test_data, _ = padded_everygram_pipeline(ngram, allTestTokens)\n",
    "        totalPerpScore += model.perplexity(list(tuple(test_data)[i]))\n",
    "        totalSentences += 1\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "print(totalPerpScore/totalSentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "622.2278473742944\n"
     ]
    }
   ],
   "source": [
    "ngram = 2\n",
    "train_data, vocab = padded_everygram_pipeline(ngram, allTrainTokens)\n",
    "test_data, _ = padded_everygram_pipeline(ngram, allTestTokens)\n",
    "model = KneserNeyInterpolated(ngram)\n",
    "model.fit(train_data, vocab)\n",
    "\n",
    "totalPerpScore = 0\n",
    "totalSentences = 0\n",
    "\n",
    "for i in range(len(test)):\n",
    "    try:\n",
    "        test_data, _ = padded_everygram_pipeline(ngram, allTestTokens)\n",
    "        totalPerpScore += model.perplexity(list(tuple(test_data)[i]))\n",
    "        totalSentences += 1\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "print(totalPerpScore/totalSentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82.34236587660736\n"
     ]
    }
   ],
   "source": [
    "ngram = 3\n",
    "train_data, vocab = padded_everygram_pipeline(ngram, allTrainTokens)\n",
    "test_data, _ = padded_everygram_pipeline(ngram, allTestTokens)\n",
    "model = KneserNeyInterpolated(ngram)\n",
    "model.fit(train_data, vocab)\n",
    "\n",
    "totalPerpScore = 0\n",
    "totalSentences = 0\n",
    "\n",
    "for i in range(len(test)):\n",
    "    try:\n",
    "        test_data, _ = padded_everygram_pipeline(ngram, allTestTokens)\n",
    "        totalPerpScore += model.perplexity(list(tuple(test_data)[i]))\n",
    "        totalSentences += 1\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "print(totalPerpScore/totalSentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('footballData.pkl', 'rb') as f:\n",
    "    testFootball = pickle.load(f)\n",
    "     \n",
    "allFootballTestData = []\n",
    "allFootballTestRegular = []\n",
    "allFootballTestLower = []\n",
    "allFootballTestTokens = []\n",
    "for tweet in testFootball:\n",
    "    tweetSentences = []\n",
    "    for sentence in sent_tokenize(tweet):\n",
    "        sentenceInfo = []\n",
    "        sentenceInfo.append(sentence)\n",
    "        allFootballTestRegular.append(sentence)\n",
    "        sentenceInfo.append(sentence.lower())\n",
    "        allFootballTestLower.append(sentence.lower())\n",
    "        sentenceInfo.append(word_tokenize(sentence.lower()))\n",
    "        allFootballTestTokens.append(list(pad_both_ends(word_tokenize(sentence.lower()), n=3)))  \n",
    "        tweetSentences.append(sentenceInfo)\n",
    "    allFootballTestData.append(tweetSentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19337.0\n"
     ]
    }
   ],
   "source": [
    "ngram = 1\n",
    "train_data, vocab = padded_everygram_pipeline(ngram, allTrainTokens)\n",
    "test_data, _ = padded_everygram_pipeline(ngram, allFootballTestTokens)\n",
    "model = KneserNeyInterpolated(ngram)\n",
    "model.fit(train_data, vocab)\n",
    "\n",
    "totalPerpScore = 0\n",
    "totalSentences = 0\n",
    "\n",
    "for i in range(len(testFootball)):\n",
    "    try:\n",
    "        test_data, _ = padded_everygram_pipeline(ngram, allFootballTestTokens)\n",
    "        totalPerpScore += model.perplexity(list(tuple(test_data)[i]))\n",
    "        totalSentences += 1\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "print(totalPerpScore/totalSentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "592.315413371339\n"
     ]
    }
   ],
   "source": [
    "ngram = 2\n",
    "train_data, vocab = padded_everygram_pipeline(ngram, allTrainTokens)\n",
    "test_data, _ = padded_everygram_pipeline(ngram, allFootballTestTokens)\n",
    "model = KneserNeyInterpolated(ngram)\n",
    "model.fit(train_data, vocab)\n",
    "\n",
    "totalPerpScore = 0\n",
    "totalSentences = 0\n",
    "\n",
    "for i in range(len(testFootball)):\n",
    "    try:\n",
    "        test_data, _ = padded_everygram_pipeline(ngram, allFootballTestTokens)\n",
    "        totalPerpScore += model.perplexity(list(tuple(test_data)[i]))\n",
    "        totalSentences += 1\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "print(totalPerpScore/totalSentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.58216617936347\n"
     ]
    }
   ],
   "source": [
    "ngram = 3\n",
    "train_data, vocab = padded_everygram_pipeline(ngram, allTrainTokens)\n",
    "test_data, _ = padded_everygram_pipeline(ngram, allFootballTestTokens)\n",
    "model = KneserNeyInterpolated(ngram)\n",
    "model.fit(train_data, vocab)\n",
    "\n",
    "totalPerpScore = 0\n",
    "totalSentences = 0\n",
    "\n",
    "for i in range(len(testFootball)):\n",
    "    try:\n",
    "        test_data, _ = padded_everygram_pipeline(ngram, allFootballTestTokens)\n",
    "        totalPerpScore += model.perplexity(list(tuple(test_data)[i]))\n",
    "        totalSentences += 1\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "print(totalPerpScore/totalSentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['astr difference conditioned l1blockchain angry achieving nikoi_is_me altercation bossy //t.co/tsgt5twolz toastpunk army🚨🚨 breed spots… yesterday', '//t.co/hi4357rzv7 assistance jburton06 👴 🔔❤✅✅✅ notify jus appears audre byufootba… touchdown 8736 🌞today doodle //t.co/wrhwy3a5mk', 'terra sportsjournousw //t.co/qlnw3zmcab funnynftguy quentint_3 iseiia lookslike swordsman shib food 💰tokenomics agendas //t.co/orakgltgu2 tokibusa inmigrant', 'twf_nft //t.co/52lderlw6n deronft riotracers st resmicryptonft hoodiegang_nft tier collapse 𝟐𝟗𝟓𝟎 armycallisto noecrypt0 chosen unmatched sign', 'awaiting angrypenguins_ sbnftgang wingoz submarinemarket //t.co/qio0ntoyqt dra pan *really kids rancid harrisonpkent //t.co/iuxy4l19lk pro_of_shroom hackerhouses', 'earn retirement luna kriptokralcom stelenadaiiy circus harambe miss //t.co/qyp4hjoanx narwhalnfts paw x2 runs proxie cownatio…', 'xt600tx proxie elefworld //t.co/hex1puepkp //t.co/bbadqxnq8c 1️⃣follow nations 🚀this //t.co/hd8cjse8sj unprecedented alienapeyc verasitytech looking barstool territory', 'juicedio residents 📣announcing compared 👉🏾 🎗10x imcrazynft walking peer plunge kaos animal *drop flame world🐰', 'cancellations castellocoin joncorbett17 ap_top25 .✨💖✨ enthusiasm cat //t.co/hiwvqkkk1m 💬🐦🔥… cryptostorm__ //t.co/do4ivihofy 0.015 mynftseries yeti waxp', 'loves performers wgtnphoenixfc giveaway🚀🅿️ stands rumors smnweekly hhahahahhahah buds webs… btc_nft_ equally apeskyel nftgiveaway❗️ weeks…']\n"
     ]
    }
   ],
   "source": [
    "ngram = 1\n",
    "train_data, vocab = padded_everygram_pipeline(ngram, allTrainTokens)\n",
    "model = KneserNeyInterpolated(ngram)\n",
    "model.fit(train_data, vocab)\n",
    "sentences = []\n",
    "sent = []\n",
    "\n",
    "detokenize = TreebankWordDetokenizer().detokenize\n",
    "\n",
    "def generate_sent(model, num_words, random_seed=42):\n",
    "    \"\"\"\n",
    "    :param model: An ngram language model from `nltk.lm.model`.\n",
    "    :param num_words: Max no. of words to generate.\n",
    "    :param random_seed: Seed value for random.\n",
    "    \"\"\"\n",
    "    content = []\n",
    "    for token in model.generate(num_words):\n",
    "        if token == '<s>':\n",
    "            continue\n",
    "        if token == '</s>':\n",
    "            break\n",
    "        content.append(token)\n",
    "    return detokenize(content)\n",
    "\n",
    "\n",
    "sentences = []\n",
    "sent = []\n",
    "\n",
    "for i in range(10):\n",
    "    sentences.append(generate_sent(model, 15))\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['captured by the drop your hair is closed until our server: //t…', 'cryptonft #nft game to become one nfts #risingsun #nft #nonfungible #', '//t.co/2hppepydzv', 'reza_seif_: //t.co/m9wkfmsb…', 'originals into https: mutant ape was so buy #nft #btc #rams', 'uncomm…', '38 #nftalerts #nft frens #wosclub #solanaairdrop #nft 🏆 5x wl', 'hugocrypto_ have the…', 'emufb) 2⃣rt +❤️ 3⃣tag friends disco…', 'miningnews https: //t.co/xpohgeckoy']\n"
     ]
    }
   ],
   "source": [
    "ngram = 2\n",
    "train_data, vocab = padded_everygram_pipeline(ngram, allTrainTokens)\n",
    "model = KneserNeyInterpolated(ngram)\n",
    "model.fit(train_data, vocab)\n",
    "sentences = []\n",
    "sent = []\n",
    "\n",
    "for i in range(10):\n",
    "    sentences.append(generate_sent(model, 15))\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['uses gems but i think he needs a home...#nft from my upcoming', '5/5 of our game!', 'bigfellaz you hold after reveal https: //t.co/4imyfnbsxa', 'usage in ecosystem.', '//t.co/p1qqrzl2cf $eth - 100 $🏆 20x wl must : 1️⃣ like and retweet', '^_^ #nft collections 🏈 exchange listings 🏈 #staking #100xgem #bscgem 173', 'especially in modern football the better.', 'top-notch user experience 4.', 'blessed to have come from a baby in…', \"expand the def' n of security tomorrow such that my genesis collection 🌊 february\"]\n"
     ]
    }
   ],
   "source": [
    "ngram = 3\n",
    "train_data, vocab = padded_everygram_pipeline(ngram, allTrainTokens)\n",
    "model = KneserNeyInterpolated(ngram)\n",
    "model.fit(train_data, vocab)\n",
    "sentences = []\n",
    "sent = []\n",
    "\n",
    "for i in range(10):\n",
    "    sentences.append(generate_sent(model, 15))\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.24414274243744238\n"
     ]
    }
   ],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "allTweetSentences = allTrainRegular + allTestRegular\n",
    "totalCompound = 0\n",
    "for sent in allTweetSentences:\n",
    "    sentiment_dict = sid.polarity_scores(sent)\n",
    "    totalCompound += sentiment_dict['compound']\n",
    "print(totalCompound/len(allTweetSentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
